{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa8e56-6854-4a93-928f-61fdae5442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_LSTM_dask.py\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∂  Imports & Dask cluster\n",
    "# -----------------------------------------------------------\n",
    "import torch\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil, json, tempfile\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import LocalCluster, Client, performance_report\n",
    "from IPython.display import display # Dask dashboard inside the notebook itself:\n",
    "import pickle\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Robust downloader\n",
    "# -----------------------------------------------------------\n",
    "import urllib.error\n",
    "\n",
    "def get_usgs_streamflow(site, start_date, end_date, min_end_date=\"2024-12-31\"):\n",
    "    \"\"\"\n",
    "    Download daily streamflow data from USGS NWIS for a given site and date range.\n",
    "    Assumes columns '20d' (date) and '14n' (flow in cfs).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame or None if download fails or structure is unexpected\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "        f\"?format=rdb&sites={site}&startDT={start_date}&endDT={end_date}\"\n",
    "        \"&parameterCd=00060&siteStatus=all\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(url, comment=\"#\", sep=\"\\t\", header=1, parse_dates=[\"20d\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[{site}] failed to download: {e}; skipping\")\n",
    "        return None\n",
    "\n",
    "    if \"14n\" not in df.columns or \"20d\" not in df.columns:\n",
    "        print(f\"[{site}] missing expected columns '20d' and '14n'; skipping\")\n",
    "        return None\n",
    "\n",
    "    df = df.rename(columns={\"14n\": \"streamflow_cfs\", \"20d\": \"date\"})\n",
    "    df[\"streamflow_cfs\"] = pd.to_numeric(df[\"streamflow_cfs\"], errors=\"coerce\")\n",
    "\n",
    "    # Remove rows with NaNs\n",
    "    df = df.dropna(subset=[\"streamflow_cfs\"])\n",
    "    if df.empty:\n",
    "        print(f\"[{site}] all streamflow data missing or invalid; skipping\")\n",
    "        return None\n",
    "\n",
    "    # Check time coverage\n",
    "    if pd.to_datetime(df[\"date\"].max()) < pd.to_datetime(min_end_date):\n",
    "        print(f\"[{site}] data ends at {df['date'].max()}, < {min_end_date}; skipping\")\n",
    "        return None\n",
    "\n",
    "    # Convert to cubic meters per second (cms)\n",
    "    df[\"streamflow_cms\"] = df[\"streamflow_cfs\"] * 0.0283168\n",
    "    df = df[[\"date\", \"streamflow_cms\"]].set_index(\"date\").sort_index()\n",
    "\n",
    "    return df\n",
    "# # Example usage:\n",
    "# # site_id = gaugeID  # Example gauge ID\n",
    "# site_id = '09085000'\n",
    "\n",
    "# start = '2015-01-01'\n",
    "# end = '2024-12-31'\n",
    "\n",
    "# streamflow_data = get_usgs_streamflow(site_id, start, end)\n",
    "# print(streamflow_data.tail())\n",
    "\n",
    "def get_or_download_streamflows(df, start_date=\"2015-01-01\", end_date=\"2024-12-31\"):\n",
    "    streamflow_file = FINAL_OUT / \"streamflows.pkl\"\n",
    "    skipped_file    = FINAL_OUT / \"skipped_gauges.txt\"\n",
    "\n",
    "    if streamflow_file.exists() and skipped_file.exists():\n",
    "        print(\"üîÅ Loading cached streamflows and skipped gauges...\")\n",
    "        with open(streamflow_file, \"rb\") as f:\n",
    "            streamflows = pickle.load(f)\n",
    "        with open(skipped_file, \"r\") as f:\n",
    "            skipped_gauges = [line.strip() for line in f]\n",
    "    else:\n",
    "        print(\"‚¨áÔ∏è  Downloading streamflows from USGS...\")\n",
    "        streamflows = {}\n",
    "        skipped_gauges = []\n",
    "        gauge_ids = df[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "\n",
    "        for g in gauge_ids:\n",
    "            dfQ = get_usgs_streamflow(g, start_date, end_date)\n",
    "            if dfQ is None:\n",
    "                skipped_gauges.append(g)\n",
    "            else:\n",
    "                streamflows[g] = dfQ\n",
    "\n",
    "        # Save results\n",
    "        FINAL_OUT.mkdir(exist_ok=True)\n",
    "        with open(streamflow_file, \"wb\") as f:\n",
    "            pickle.dump(streamflows, f)\n",
    "        with open(skipped_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(skipped_gauges))\n",
    "        print(f\"‚úÖ Saved streamflows to {streamflow_file}\")\n",
    "        print(f\"‚ùå Saved skipped gauges to {skipped_file}\")\n",
    "        \n",
    "    return streamflows, skipped_gauges\n",
    "\n",
    "\n",
    "# CAMELS basins\n",
    "df=gpd.read_file('/Projects/HydroMet/currierw/Caravan-Jan25-csv/shapefiles/camels/camels_basin_shapes.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137b1ee-57f7-4a08-b0ea-971a80231d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, multiprocessing, torch, psutil\n",
    "\n",
    "print(\"Logical CPU cores :\", multiprocessing.cpu_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device      :\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU capability   :\", torch.cuda.get_device_capability(0))\n",
    "else:\n",
    "    print(\"No CUDA‚Äëcapable GPU detected\")\n",
    "\n",
    "print(\"RAM (GB total)   :\", round(psutil.virtual_memory().total / 1e9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45753f-0444-450a-91fe-8ae19f42b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# ‚ù∑  Constants & helpers\n",
    "# -----------------------------------------------------------\n",
    "BASE_OBS  = Path('/Projects/HydroMet/currierw/ERA5_LAND')\n",
    "BASE_FCST = Path('/Projects/HydroMet/currierw/HRES')\n",
    "SCRATCH   = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')  # will be recreated\n",
    "FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "\n",
    "FORECAST_BLOCKS = {\n",
    "    \"train\":      pd.date_range('2016-01-01', '2020-09-30', freq='5D'),\n",
    "    \"validation\": pd.date_range('2020-10-01', '2022-09-30', freq='5D'),\n",
    "    \"test\":       pd.date_range('2022-10-01', '2024-09-30', freq='5D'),\n",
    "}\n",
    "\n",
    "REQUIRED_KEYS = ['precip', 'temp', 'net_solar', 'flow', 'target']\n",
    "EXPECTED_LEN  = 106          # enforce the length we know is correct\n",
    "\n",
    "# ERA5_ZARR = None\n",
    "# HRES_ZARR = None\n",
    "\n",
    "print(\"Opening Zarr datasets once and broadcasting to workers...\")\n",
    "ERA5_ZARR = xr.open_zarr(BASE_OBS / 'camels_rechunked.zarr', consolidated=True, chunks={})\n",
    "HRES_ZARR = xr.open_zarr(BASE_FCST / 'camels_rechunked.zarr', consolidated=True, decode_timedelta=True, chunks={})\n",
    "\n",
    "def standardize_tensor(arr, mean, std):\n",
    "    return (arr - mean) / std\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∏  Per‚Äìgauge worker\n",
    "# -----------------------------------------------------------\n",
    "# @delayed\n",
    "import time\n",
    "def process_block(gauge_id, df_streamflow, split, fcst_dates, ERA5_ZARR, HRES_ZARR):\n",
    "    t0 = time.time()\n",
    "\n",
    "    out_files = []\n",
    "    try:\n",
    "        ds_obs = ERA5_ZARR.sel(basin=f'camels_{gauge_id}')\n",
    "        ds_fcst = HRES_ZARR.sel(basin=f'camels_{gauge_id}')\n",
    "\n",
    "        ds_obs_p = ds_obs['era5land_total_precipitation'].sel(date=slice('2015','2024-09-30'))\n",
    "        ds_obs_t = ds_obs['era5land_temperature_2m'].sel(date=slice('2015','2024-09-30'))\n",
    "        ds_obs_s = ds_obs['era5land_surface_net_solar_radiation'].sel(date=slice('2015','2024-09-30'))\n",
    "\n",
    "        samples = []\n",
    "        for fcst_date in fcst_dates:\n",
    "            try:\n",
    "                start_weekly = fcst_date - pd.Timedelta(days=305)\n",
    "                end_weekly = fcst_date - pd.Timedelta(days=60) - pd.Timedelta(days=1)\n",
    "                start_daily = fcst_date - pd.Timedelta(days=60)\n",
    "                end_daily = fcst_date - pd.Timedelta(days=1)\n",
    "                start_fore = fcst_date\n",
    "                end_fore = fcst_date + pd.Timedelta(days=10)\n",
    "\n",
    "                q_weekly = df_streamflow.loc[start_weekly:end_weekly]['streamflow_cms'].resample('7D').mean()\n",
    "                q_daily  = df_streamflow.loc[start_daily:end_daily]['streamflow_cms']\n",
    "                q_fore   = df_streamflow.loc[start_fore:end_fore]['streamflow_cms']\n",
    "                q_combined = pd.concat([q_weekly, q_daily, q_fore]).to_xarray()\n",
    "                q_combined.name = 'streamflow'\n",
    "\n",
    "                obs_weekly_p = ds_obs_p.sel(date=slice(start_weekly, end_weekly)).resample(date='7D').mean()\n",
    "                obs_weekly_t = ds_obs_t.sel(date=slice(start_weekly, end_weekly)).resample(date='7D').mean()\n",
    "                obs_weekly_s = ds_obs_s.sel(date=slice(start_weekly, end_weekly)).resample(date='7D').mean()\n",
    "\n",
    "                obs_daily_p  = ds_obs_p.sel(date=slice(start_daily, end_daily +  pd.Timedelta(days=1)))\n",
    "                obs_daily_t  = ds_obs_t.sel(date=slice(start_daily, end_daily +  pd.Timedelta(days=1)))\n",
    "                obs_daily_s  = ds_obs_s.sel(date=slice(start_daily, end_daily +  pd.Timedelta(days=1)))\n",
    "\n",
    "                tmp  = ds_fcst.sel(date=fcst_date, method='nearest')\n",
    "                fcst_dates_expand = pd.Timestamp(tmp.date.values) + pd.to_timedelta(tmp.lead_time)\n",
    "                tmp  = tmp.assign_coords(date=('lead_time', fcst_dates_expand))\n",
    "                fcst = (tmp.swap_dims({'lead_time':'date'}).drop_vars('lead_time').isel(date=slice(0,10)))\n",
    "                fcst_p = fcst['hres_total_precipitation']\n",
    "                fcst_t = fcst['hres_temperature_2m']\n",
    "                fcst_s = fcst['hres_surface_net_solar_radiation']\n",
    "\n",
    "                precip = xr.concat([obs_weekly_p, obs_daily_p, fcst_p], dim='date')\n",
    "                temp   = xr.concat([obs_weekly_t, obs_daily_t, fcst_t], dim='date')\n",
    "                nsrad  = xr.concat([obs_weekly_s, obs_daily_s, fcst_s], dim='date')\n",
    "\n",
    "                if precip.shape[0] != EXPECTED_LEN or q_combined.shape[0] != EXPECTED_LEN:\n",
    "                    continue\n",
    "\n",
    "                flags = np.concatenate([\n",
    "                    np.full(obs_weekly_p.date.size, 0),\n",
    "                    np.full(obs_daily_p.date.size, 1),\n",
    "                    np.full(fcst_p.date.size, 2)\n",
    "                ])\n",
    "\n",
    "                sample = {\n",
    "                    'precip': precip.values.astype(np.float32),\n",
    "                    'temp':   temp.values.astype(np.float32),\n",
    "                    'net_solar': nsrad.values.astype(np.float32),\n",
    "                    'flag':   flags.astype(np.int8),\n",
    "                    'flow':   q_combined.values.astype(np.float32),\n",
    "                    'target': q_combined.values.astype(np.float32),\n",
    "                    'basin_id': gauge_id,\n",
    "                    'forecast_date': fcst_date.strftime('%Y-%m-%d')\n",
    "                }\n",
    "                samples.append(sample)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{gauge_id}] skip {fcst_date:%Y-%m-%d}: {e}\")\n",
    "        print(f\"[{gauge_id}] block processed in {time.time() - t0:.2f}s\")\n",
    "\n",
    "        if samples:\n",
    "            ds = samples_to_xarray(samples)\n",
    "            outfile = SCRATCH / f'{split}_{gauge_id}_{fcst_dates[0].strftime(\"%Y%m%d\")}.nc'\n",
    "            ds.to_netcdf(outfile)\n",
    "            out_files.append(str(outfile))\n",
    "    except Exception as e:\n",
    "        print(f\"[{gauge_id}] failed with error: {e}\")\n",
    "    return out_files\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ‚ùπ  Small helper: samples ‚Üí xarray (single gauge, single split)\n",
    "# -----------------------------------------------------------\n",
    "def samples_to_xarray(samples):\n",
    "    n = len(samples)\n",
    "    dyn = np.zeros((n, EXPECTED_LEN, 4), np.float32)\n",
    "    tgt = np.zeros((n, EXPECTED_LEN, 1), np.float32)\n",
    "    bas = np.empty(n, 'U20')\n",
    "    fct = np.empty(n, 'U20')\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        dyn[i,:,0] = s['precip']\n",
    "        dyn[i,:,1] = s['temp']\n",
    "        dyn[i,:,2] = s['net_solar']\n",
    "        dyn_inputs[i, :, 3] = s['flag'].astype(np.float32)  # must be a float for LSTM\n",
    "        tgt[i,:,0] = s['target']\n",
    "        bas[i] = s['basin_id']\n",
    "        fct[i] = s['forecast_date']\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"dynamic_inputs\": (\n",
    "                [\"sample\", \"time\", \"feature\"],\n",
    "                dyn_inputs,\n",
    "                {\"feature\": [\"precip\", \"temp\", \"net_solar\", \"flag\"]}\n",
    "            ),\n",
    "            \"targets\": (\n",
    "                [\"sample\", \"time\", \"target\"],\n",
    "                targets,\n",
    "                {\"target\": [\"streamflow\"]}\n",
    "            ),\n",
    "            \"basin_id\": ([\"sample\"], basin_ids),\n",
    "            \"forecast_date\": ([\"sample\"], forecast_dates)\n",
    "        }\n",
    "    )\n",
    "    # Add metadata describing the meaning of flag values\n",
    "    ds.attrs[\"flag_description\"] = {\n",
    "        \"0\": \"weekly reanalysis (ERA5)\",\n",
    "        \"1\": \"daily reanalysis (ERA5)\",\n",
    "        \"2\": \"forecast (HRES)\"\n",
    "    }\n",
    "    return ds\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∫  Parent / driver\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    # ---------- (re-)create scratch folder ----------\n",
    "    if SCRATCH.exists():\n",
    "        shutil.rmtree(SCRATCH)\n",
    "    SCRATCH.mkdir(parents=True)\n",
    "\n",
    "    # ---------- build gauge list ----------\n",
    "    gauge_ids = df[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "    print('have gauge list')\n",
    "    # ---------- download streamflow (skip bad gauges) ----------\n",
    "    streamflows, skipped_gauges = get_or_download_streamflows(df)\n",
    "    import sys\n",
    "    print(str(sys.getsizeof(streamflows['01013500'])*1e-6)+' mb')\n",
    "    print(f\"‚úÖ {len(streamflows)} gauges ready, ‚ùå {len(skipped_gauges)} skipped\")\n",
    "    print('Got the Gauges: starting parallelization')\n",
    "\n",
    "    # ---------- start Dask cluster ----------\n",
    "    cluster = LocalCluster(n_workers=64, threads_per_worker=1,\n",
    "                           processes=True, memory_limit=\"4GB\")\n",
    "    client  = Client(cluster)\n",
    "    display(client)  # shows dashboard link in Jupyter\n",
    "\n",
    "    # ---------- dispatch only valid gauges ----------\n",
    "    CHUNK = 100  # try 20, 40, 60 etc.\n",
    "    \n",
    "    def chunks(seq, n):\n",
    "        for i in range(0, len(seq), n):\n",
    "            yield seq[i:i + n]\n",
    "    \n",
    "    futures = []\n",
    "    \n",
    "    some_gauges = list(streamflows.keys())[:64]  # or full list if ready\n",
    "\n",
    "    scattered_streamflows = {\n",
    "        g: client.scatter(dfQ, broadcast=False) for g, dfQ in streamflows.items()\n",
    "    }\n",
    "\n",
    "\n",
    "    ERA5_ZARR_scattered = client.scatter(ERA5_ZARR, broadcast=True, hash=False)\n",
    "    HRES_ZARR_scattered = client.scatter(HRES_ZARR, broadcast=True, hash=False)\n",
    "    # broadcast=True: ensures all workers get a copy.\n",
    "    # hash=False: prevents Dask from trying to hash the big object (xarray datasets can be large and non-hashable).\n",
    "\n",
    "    for g in some_gauges:\n",
    "        dfQ = scattered_streamflows[g]\n",
    "        for split, dates in FORECAST_BLOCKS.items():\n",
    "            for sub_dates in chunks(dates, CHUNK):\n",
    "                print(f\"[submit] gauge: {g}, split: {split}, {sub_dates[0]} to {sub_dates[-1]}\")\n",
    "                fut = client.submit(process_block, g, dfQ, split, sub_dates, ERA5_ZARR_scattered, HRES_ZARR_scattered, pure=False)\n",
    "                futures.append(fut)\n",
    "    \n",
    "    print(\"All submits done\")\n",
    "    with performance_report(filename=\"report.html\"):\n",
    "        client.gather(futures)\n",
    "\n",
    "    print(\"Finished Dask computation\")\n",
    "\n",
    "\n",
    "    # ---------- concatenate split‚Äëlevel files ----------\n",
    "    print('concatenating split-level files from parallelization')\n",
    "    for split in ['train','validation','test']:\n",
    "        files = sorted(SCRATCH.glob(f'{split}_*.nc'))\n",
    "        if not files: continue\n",
    "        ds = xr.open_mfdataset(files, combine='nested',\n",
    "                               concat_dim='sample',\n",
    "                               parallel=True)\n",
    "        ds.to_netcdf(\n",
    "            FINAL_OUT / f\"{split}_data_ERA5_HRES_CAMELS_unstandardized.nc\",\n",
    "            encoding={\n",
    "                \"dynamic_inputs\": {\"zlib\": True, \"complevel\": 4},\n",
    "                \"targets\":        {\"zlib\": True, \"complevel\": 4}\n",
    "            },\n",
    "        )\n",
    "        print(f\"[‚úì] wrote {split} set ({len(files)} gauges)\")\n",
    "\n",
    "    # ---------- at the very end ----------\n",
    "    if skipped_gauges:\n",
    "        print(\"Skipped gauges:\", \", \".join(skipped_gauges))\n",
    "        with open(FINAL_OUT / \"skipped_gauges.txt\", \"w\") as fp:\n",
    "            fp.write(\"\\n\".join(skipped_gauges))\n",
    "            \n",
    "    print(\"All done.  See NetCDFs in\", FINAL_OUT)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "#     # This is only needed if you're running the script directly from a terminal or !python ‚Äî not inside a notebook.\n",
    "#     # This design allows the file to serve two purposes:\n",
    "#     # Be run directly as a standalone program.\n",
    "#     # Be imported as a library/module into another script without executing the main logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b47d76-5b62-4b28-83e7-3e0eaeda598c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology editable",
   "language": "Python",
   "name": "neuralhydrology_edit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
