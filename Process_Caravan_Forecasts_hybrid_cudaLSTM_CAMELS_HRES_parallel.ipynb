{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d137b1ee-57f7-4a08-b0ea-971a80231d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Logical CPU cores :\", multiprocessing.cpu_count())\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"CUDA device      :\", torch.cuda.get_device_name(0))\n",
    "#     print(\"GPU capability   :\", torch.cuda.get_device_capability(0))\n",
    "# else:\n",
    "#     print(\"No CUDA‚Äëcapable GPU detected\")\n",
    "\n",
    "# print(\"RAM (GB total)   :\", round(psutil.virtual_memory().total / 1e9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c45753f-0444-450a-91fe-8ae19f42b843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading cached streamflows and skipped gauges...\n",
      "Loaded 587 gauges. Skipped 84.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40507 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting batch of 500 tasks...\n",
      "Submitting batch of 500 tasks...\n",
      "Submitting batch of 500 tasks...\n",
      "Waiting for results and saving intermediate files...\n",
      "[500] Saving intermediate results at 3.0s\n",
      "[1000] Saving intermediate results at 9.3s\n",
      "Final save of remaining results...\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01423000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01434025.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01434025.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01434025.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01435000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01435000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01435000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01439500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01439500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01439500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01440000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01440000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01440000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01440400.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01440400.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01440400.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01451800.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01451800.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01451800.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01466500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01466500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01466500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01484100.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01484100.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01484100.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01485500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01485500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01485500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01486000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01486000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01486000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01487000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01487000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01487000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01491000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01491000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01491000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01510000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01510000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01510000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01516500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01516500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01516500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01518862.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01518862.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01518862.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01532000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01532000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01532000.nc\n",
      "Done. Total time: 0.3 minutes\n",
      "Results saved to /Projects/HydroMet/currierw/HRES_processed_tmp\n"
     ]
    }
   ],
   "source": [
    "# preprocess_LSTM_dask.py\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∂  Imports & Dask cluster\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd, numpy as np, xarray as xr, geopandas as gpd\n",
    "import traceback, sys, dask, pickle, torch, zarr\n",
    "import multiprocessing, psutil, os, tempfile, json, shutil, time, urllib.error\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import LocalCluster, Client, performance_report, wait, as_completed\n",
    "from IPython.display import display # Dask dashboard inside the notebook itself:\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from data_access import era5, hres\n",
    "from utils import build_sample, samples_to_xarray, build_sample_wrapped, get_usgs_streamflow, get_or_download_streamflows\n",
    "# from config import FORECAST_BLOCKS, SCRATCH\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# paths & constants -------------------------------------------------\n",
    "BASE_OBS  = Path('/Projects/HydroMet/currierw/ERA5_LAND')\n",
    "BASE_FCST = Path('/Projects/HydroMet/currierw/HRES')\n",
    "SCRATCH   = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')\n",
    "FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "STREAMFLOW_PATH = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "\n",
    "FORECAST_BLOCKS = {\n",
    "    \"train\":      pd.date_range('2016-01-01', '2020-09-30', freq='5D'),\n",
    "    \"validation\": pd.date_range('2020-10-01', '2022-09-30', freq='5D'),\n",
    "    \"test\":       pd.date_range('2022-10-01', '2024-09-30', freq='5D'),\n",
    "}\n",
    "EXPECTED_LEN = 106\n",
    "\n",
    "\n",
    "# ---------------------------- Main driver -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 500 # This is the number of Dask tasks (i.e., futures) you submit before pausing and waiting for them to finish.\n",
    "    SAVE_INTERVAL = 500 # This determines how often you write results to disk (as NetCDF) while processing the completed futures.\n",
    "\n",
    "    # ---- Clear scratch output folder ----\n",
    "    if SCRATCH.exists():\n",
    "        shutil.rmtree(SCRATCH)\n",
    "    SCRATCH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Load gauges ----\n",
    "    gdf = gpd.read_file('/Projects/HydroMet/currierw/Caravan-Jan25-csv/shapefiles/camels/camels_basin_shapes.shp')\n",
    "    gauge_ids = gdf[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "    streamflows, skipped = get_or_download_streamflows(gdf, STREAMFLOW_PATH)\n",
    "\n",
    "    print(f\"Loaded {len(streamflows)} gauges. Skipped {len(skipped)}.\")\n",
    "\n",
    "    # ---- Dask cluster setup ----\n",
    "    module_path = '/home/wcurrier/LSTM_runoff_tests'\n",
    "\n",
    "    def add_module_path(dask_worker):\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, module_path)\n",
    "\n",
    "    cluster = LocalCluster(n_workers=16, threads_per_worker=2, memory_limit=\"8GB\")\n",
    "    # cluster.adapt(minimum=8, maximum=32)\n",
    "    client = Client(cluster)\n",
    "    client.run(add_module_path)\n",
    "\n",
    "    # ---- Task submission in batches ----\n",
    "\n",
    "    ds_era5 = era5()\n",
    "    ds_hres = hres()\n",
    "\n",
    "    all_futures = [] # future object is something that will be done in the background. \n",
    "                     # We don't have the result yet but we've told Dask to compute this \n",
    "                     # with client.submit(...)\n",
    "    pending = []     # This is just a Python list where you collect all the Future objects \n",
    "                     # before submitting them in a batch.\n",
    "\n",
    "    for gauge_id in list(streamflows.keys())[:50]: # loop over gauges\n",
    "        dfQ = streamflows[gauge_id]\n",
    "        for split, dates in FORECAST_BLOCKS.items(): # loop over forecast dates and train/val/test\n",
    "            for d in dates[:10]: # only a subset of dates for each split\n",
    "                future = client.submit(\n",
    "                    build_sample_wrapped, gauge_id, split, d, dfQ, ds_era5, ds_hres, pure=False\n",
    "                )\n",
    "                # build_sample_wrapped eventually called build_sample which creates the data we're interested in\n",
    "                pending.append(future)\n",
    "\n",
    "                if len(pending) >= BATCH_SIZE:\n",
    "                    print(f\"Submitting batch of {len(pending)} tasks...\")\n",
    "                    wait(pending, timeout=None)\n",
    "                    all_futures.extend(pending) # a complete list of all futures we've launched so far\n",
    "                    pending = []\n",
    "\n",
    "    # Final pending batch\n",
    "    if pending:\n",
    "        print(f\"Submitting final batch of {len(pending)} tasks...\")\n",
    "        wait(pending, timeout=None)\n",
    "        all_futures.extend(pending)\n",
    "\n",
    "    # ---- Collect results as they complete ----\n",
    "    print(\"Waiting for results and saving intermediate files...\")\n",
    "    start_time = time.time()\n",
    "    grouped = defaultdict(list)\n",
    "    counter = 0\n",
    "\n",
    "    # This loop will:\n",
    "    # - Wait for each Future to finish.\n",
    "    # - Collect the result (the dictionary).\n",
    "    # - Group them for writing out to disk (as NetCDF).\n",
    "    for future, result in as_completed(all_futures, with_results=True):\n",
    "        if result is not None:\n",
    "            grouped[(result[\"basin_id\"], result[\"split\"])].append(result)\n",
    "            counter += 1\n",
    "\n",
    "        if counter % SAVE_INTERVAL == 0 and counter > 0:\n",
    "            # After we collect and group results from as_completed(all_futures, with_results=True):\n",
    "            # Every SAVE_INTERVAL samples, flush everything to disk and clear memory.\n",
    "            # - Avoids memory bloat: You don‚Äôt hold all samples in RAM\n",
    "            # - Protects against crashes: If your run dies mid-process, you‚Äôve already saved partial results\n",
    "            # - Reduces I/O contention: Writing files constantly (e.g., every sample) can slow things down\n",
    "            \n",
    "            print(f\"[{counter}] Saving intermediate results at {time.time() - start_time:.1f}s\")\n",
    "            for (gauge, split), samples in grouped.items():\n",
    "                ds = samples_to_xarray(samples)\n",
    "                out = SCRATCH / f\"{split}_{gauge}_{counter}.nc\"\n",
    "\n",
    "                if ds.dynamic_inputs.isnull().any() or ds.targets.isnull().any():\n",
    "                    print(f\"[{split}][{gauge}] contains NaNs ‚Äî skipping {out.name}\")\n",
    "                    continue\n",
    "\n",
    "                ds.to_netcdf(out)                \n",
    "                # print(\"Wrote\", out)\n",
    "\n",
    "            grouped.clear()\n",
    "\n",
    "    # ---- Final save ----\n",
    "    print(\"Final save of remaining results...\")\n",
    "    summary = []\n",
    "\n",
    "    for (gauge, split), samples in grouped.items():\n",
    "        ds = samples_to_xarray(samples)\n",
    "        out = SCRATCH / f\"{split}_{gauge}.nc\"\n",
    "\n",
    "        if ds.dynamic_inputs.isnull().any() or ds.targets.isnull().any():\n",
    "            print(f\"[{split}][{gauge}] contains NaNs ‚Äî skipping final save\")\n",
    "            continue\n",
    "\n",
    "        ds.to_netcdf(out)\n",
    "        print(\"Wrote\", out)\n",
    "\n",
    "        summary.append({\n",
    "            \"gauge_id\": gauge,\n",
    "            \"split\": split,\n",
    "            \"n_samples\": len(samples),\n",
    "            \"filename\": out.name\n",
    "        })\n",
    "\n",
    "    # ---- Summary output ----\n",
    "    pd.DataFrame(summary).to_csv(\"processed_summary.csv\", index=False)\n",
    "    print(f\"Done. Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"Results saved to {SCRATCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafb288-9c8c-4db9-a59e-0ecfcb2cc319",
   "metadata": {},
   "source": [
    "# Concatenate indivudal files from parallel output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac797ee9-6771-4a73-8daf-eb5462ace704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# SCRATCH = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')\n",
    "# FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "# # Step 1: List of splits\n",
    "# splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# for split in splits:\n",
    "#     files = sorted(SCRATCH.glob(f\"{split}_*.nc\"))\n",
    "#     valid_datasets = []\n",
    "\n",
    "#     print(f\"\\nüß™ Checking {len(files)} files for '{split}' split...\")\n",
    "\n",
    "#     for f in files:\n",
    "#         ds = xr.open_dataset(f)\n",
    "\n",
    "#         # Check for NaNs\n",
    "#         has_nan = (\n",
    "#             np.isnan(ds[\"dynamic_inputs\"]).any() or\n",
    "#             np.isnan(ds[\"targets\"]).any()\n",
    "#         )\n",
    "\n",
    "#         if has_nan:\n",
    "#             print(f\"‚ö†Ô∏è Skipping file with NaNs: {f.name}\")\n",
    "#             continue\n",
    "\n",
    "#         valid_datasets.append(ds)\n",
    "\n",
    "#     if not valid_datasets:\n",
    "#         print(f\"‚ùå No valid files found for split: {split}\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"‚úÖ {len(valid_datasets)} valid files found for '{split}'\")\n",
    "\n",
    "#     # Step 2: Concatenate along the sample dimension\n",
    "#     combined = xr.concat(valid_datasets, dim=\"sample\")\n",
    "\n",
    "#     # Step 3: Write out combined file\n",
    "#     output_path = FINAL_OUT / f\"{split}.nc\"\n",
    "#     combined.to_netcdf(output_path)\n",
    "#     print(f\"üì¶ Wrote combined dataset for '{split}' ‚Üí {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff4f7c-5689-4e53-9fb1-f9d40956f367",
   "metadata": {},
   "source": [
    "# Cleanup scratch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b47d76-5b62-4b28-83e7-3e0eaeda598c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Optional cleanup\n",
    "# for f in SCRATCH.glob(\"*.nc\"):\n",
    "#     f.unlink()  # or f.rename(SCRATCH / \"archive\" / f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology editable new",
   "language": "Python",
   "name": "neuralhydrology_edit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
