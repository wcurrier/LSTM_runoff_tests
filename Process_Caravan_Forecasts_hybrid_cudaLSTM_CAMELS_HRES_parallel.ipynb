{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fa8e56-6854-4a93-928f-61fdae5442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_LSTM_dask.py\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∂  Imports & Dask cluster\n",
    "# -----------------------------------------------------------\n",
    "import torch\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil, json, tempfile\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import LocalCluster, Client, performance_report\n",
    "from IPython.display import display # Dask dashboard inside the notebook itself:\n",
    "import pickle\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Robust downloader\n",
    "# -----------------------------------------------------------\n",
    "import urllib.error\n",
    "\n",
    "def get_usgs_streamflow(site, start_date, end_date, min_end_date=\"2024-12-31\"):\n",
    "    \"\"\"\n",
    "    Download daily streamflow data from USGS NWIS for a given site and date range.\n",
    "    Assumes columns '20d' (date) and '14n' (flow in cfs).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame or None if download fails or structure is unexpected\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "        f\"?format=rdb&sites={site}&startDT={start_date}&endDT={end_date}\"\n",
    "        \"&parameterCd=00060&siteStatus=all\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(url, comment=\"#\", sep=\"\\t\", header=1, parse_dates=[\"20d\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[{site}] failed to download: {e}; skipping\")\n",
    "        return None\n",
    "\n",
    "    if \"14n\" not in df.columns or \"20d\" not in df.columns:\n",
    "        print(f\"[{site}] missing expected columns '20d' and '14n'; skipping\")\n",
    "        return None\n",
    "\n",
    "    df = df.rename(columns={\"14n\": \"streamflow_cfs\", \"20d\": \"date\"})\n",
    "    df[\"streamflow_cfs\"] = pd.to_numeric(df[\"streamflow_cfs\"], errors=\"coerce\")\n",
    "\n",
    "    # Remove rows with NaNs\n",
    "    df = df.dropna(subset=[\"streamflow_cfs\"])\n",
    "    if df.empty:\n",
    "        print(f\"[{site}] all streamflow data missing or invalid; skipping\")\n",
    "        return None\n",
    "\n",
    "    # Check time coverage\n",
    "    if pd.to_datetime(df[\"date\"].max()) < pd.to_datetime(min_end_date):\n",
    "        print(f\"[{site}] data ends at {df['date'].max()}, < {min_end_date}; skipping\")\n",
    "        return None\n",
    "\n",
    "    # Convert to cubic meters per second (cms)\n",
    "    df[\"streamflow_cms\"] = df[\"streamflow_cfs\"] * 0.0283168\n",
    "    df = df[[\"date\", \"streamflow_cms\"]].set_index(\"date\").sort_index()\n",
    "\n",
    "    return df\n",
    "# # Example usage:\n",
    "# # site_id = gaugeID  # Example gauge ID\n",
    "# site_id = '09085000'\n",
    "\n",
    "# start = '2015-01-01'\n",
    "# end = '2024-12-31'\n",
    "\n",
    "# streamflow_data = get_usgs_streamflow(site_id, start, end)\n",
    "# print(streamflow_data.tail())\n",
    "\n",
    "def get_or_download_streamflows(df, start_date=\"2015-01-01\", end_date=\"2024-12-31\"):\n",
    "    streamflow_file = FINAL_OUT / \"streamflows.pkl\"\n",
    "    skipped_file    = FINAL_OUT / \"skipped_gauges.txt\"\n",
    "\n",
    "    if streamflow_file.exists() and skipped_file.exists():\n",
    "        print(\"üîÅ Loading cached streamflows and skipped gauges...\")\n",
    "        with open(streamflow_file, \"rb\") as f:\n",
    "            streamflows = pickle.load(f)\n",
    "        with open(skipped_file, \"r\") as f:\n",
    "            skipped_gauges = [line.strip() for line in f]\n",
    "    else:\n",
    "        print(\"‚¨áÔ∏è  Downloading streamflows from USGS...\")\n",
    "        streamflows = {}\n",
    "        skipped_gauges = []\n",
    "        gauge_ids = df[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "\n",
    "        for g in gauge_ids:\n",
    "            dfQ = get_usgs_streamflow(g, start_date, end_date)\n",
    "            if dfQ is None:\n",
    "                skipped_gauges.append(g)\n",
    "            else:\n",
    "                streamflows[g] = dfQ\n",
    "\n",
    "        # Save results\n",
    "        FINAL_OUT.mkdir(exist_ok=True)\n",
    "        with open(streamflow_file, \"wb\") as f:\n",
    "            pickle.dump(streamflows, f)\n",
    "        with open(skipped_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(skipped_gauges))\n",
    "        print(f\"‚úÖ Saved streamflows to {streamflow_file}\")\n",
    "        print(f\"‚ùå Saved skipped gauges to {skipped_file}\")\n",
    "        \n",
    "    return streamflows, skipped_gauges\n",
    "\n",
    "\n",
    "# CAMELS basins\n",
    "df=gpd.read_file('/Projects/HydroMet/currierw/Caravan-Jan25-csv/shapefiles/camels/camels_basin_shapes.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d137b1ee-57f7-4a08-b0ea-971a80231d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical CPU cores : 256\n",
      "CUDA device      : NVIDIA A100-SXM4-40GB\n",
      "GPU capability   : (8, 0)\n",
      "RAM (GB total)   : 1082.0\n"
     ]
    }
   ],
   "source": [
    "import os, multiprocessing, torch, psutil\n",
    "\n",
    "print(\"Logical CPU cores :\", multiprocessing.cpu_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device      :\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU capability   :\", torch.cuda.get_device_capability(0))\n",
    "else:\n",
    "    print(\"No CUDA‚Äëcapable GPU detected\")\n",
    "\n",
    "print(\"RAM (GB total)   :\", round(psutil.virtual_memory().total / 1e9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c45753f-0444-450a-91fe-8ae19f42b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# ‚ù∑  Constants & helpers\n",
    "# -----------------------------------------------------------\n",
    "BASE_OBS  = Path('/Projects/HydroMet/currierw/ERA5_LAND')\n",
    "BASE_FCST = Path('/Projects/HydroMet/currierw/HRES')\n",
    "SCRATCH   = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')  # will be recreated\n",
    "FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "\n",
    "FORECAST_BLOCKS = {\n",
    "    \"train\":      pd.date_range('2016-01-01', '2020-09-30', freq='5D'),\n",
    "    \"validation\": pd.date_range('2020-10-01', '2022-09-30', freq='5D'),\n",
    "    \"test\":       pd.date_range('2022-10-01', '2024-09-30', freq='5D'),\n",
    "}\n",
    "\n",
    "REQUIRED_KEYS = ['precip', 'temp', 'net_solar', 'flow', 'target']\n",
    "EXPECTED_LEN  = 115          # enforce the length we know is correct\n",
    "\n",
    "def standardize_tensor(arr, mean, std):\n",
    "    return (arr - mean) / std\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∏  Per‚Äìgauge worker\n",
    "# -----------------------------------------------------------\n",
    "# @delayed\n",
    "def process_gauge(gauge_id, df_streamflow):\n",
    "    # print(f\"[{gauge_id}] Starting gauge\")\n",
    "    print(\"process_gauge called with gauge_id:\", gauge_id)\n",
    "    print(\"process_gauge called with df_streamflow type:\", type(df_streamflow))\n",
    "    \"\"\"\n",
    "    Reads ERA5‚ÄëLAND + HRES forecasts for one gauge,\n",
    "    builds samples, writes temporary NetCDF files,\n",
    "    and returns running statistics for scaling.\n",
    "    \"\"\"\n",
    "    out_files = []\n",
    "\n",
    "    try:\n",
    "        ds_obs = xr.open_zarr(BASE_OBS / 'timeseries.zarr',\n",
    "                              consolidated=True,\n",
    "                              chunks={}).sel(basin=f'camels_{gauge_id}')\n",
    "\n",
    "        ds_fcst = xr.open_zarr(BASE_FCST / 'timeseries.zarr',\n",
    "                               consolidated=True,\n",
    "                               decode_timedelta=True,\n",
    "                               chunks={}).sel(basin=f'camels_{gauge_id}')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[{gauge_id}] missing Zarr store ‚Äì skipping\")\n",
    "        return out_files   # empty\n",
    "\n",
    "    # Slice obs once (cheap view)\n",
    "    ds_obs_p = ds_obs['era5land_total_precipitation'].sel(date=slice('2015','2024-09-30'))\n",
    "    ds_obs_t = ds_obs['era5land_temperature_2m'].sel(date=slice('2015','2024-09-30'))\n",
    "    ds_obs_s = ds_obs['era5land_surface_net_solar_radiation'].sel(date=slice('2015','2024-09-30'))\n",
    "\n",
    "    for split, fcst_dates in FORECAST_BLOCKS.items():\n",
    "        samples = []\n",
    "        for fcst_date in fcst_dates:\n",
    "            try:\n",
    "                # Windows\n",
    "                start_weekly  = fcst_date - pd.Timedelta(days=294)\n",
    "                end_weekly    = fcst_date - pd.Timedelta(days=60) - pd.Timedelta(days=1)\n",
    "                start_daily   = fcst_date - pd.Timedelta(days=60)\n",
    "                end_daily     = fcst_date - pd.Timedelta(days=1)\n",
    "                start_fore    = fcst_date\n",
    "                end_fore      = fcst_date + pd.Timedelta(days=10)\n",
    "\n",
    "                # Streamflow (caller passes df_streamflow to avoid re‚Äëdownloading)\n",
    "                q_weekly = (df_streamflow.loc[start_weekly:end_weekly]\n",
    "                            ['streamflow_cms']\n",
    "                            .resample('W-SUN', label='left', closed='left')\n",
    "                            .mean())\n",
    "                q_daily  = df_streamflow.loc[start_daily:end_daily]['streamflow_cms']\n",
    "                q_fore   = df_streamflow.loc[start_fore:end_fore]['streamflow_cms']\n",
    "                q_combined = pd.concat([q_weekly, q_daily, q_fore]).to_xarray()\n",
    "                q_combined.name = 'streamflow'\n",
    "\n",
    "                # ERA5 weekly / daily\n",
    "                obs_weekly_p = ds_obs_p.sel(date=slice(start_weekly, end_weekly)).resample(date='7D').mean()\n",
    "                obs_weekly_t = ds_obs_t.sel(date=slice(start_weekly, end_weekly)).resample(date='7D').mean()\n",
    "                obs_weekly_s = ds_obs_s.sel(date=slice(start_weekly, end_weekly)).resample(date='7D').mean()\n",
    "\n",
    "                obs_daily_p  = ds_obs_p.sel(date=slice(start_daily, end_daily))\n",
    "                obs_daily_t  = ds_obs_t.sel(date=slice(start_daily, end_daily))\n",
    "                obs_daily_s  = ds_obs_s.sel(date=slice(start_daily, end_daily))\n",
    "\n",
    "                # HRES forecast\n",
    "                tmp  = ds_fcst.sel(date=fcst_date, method='nearest')\n",
    "                fcst_dates_expand = pd.Timestamp(tmp.date.values) + pd.to_timedelta(tmp.lead_time)\n",
    "                tmp  = tmp.assign_coords(date=('lead_time', fcst_dates_expand))\n",
    "                fcst = (tmp.swap_dims({'lead_time':'date'})\n",
    "                           .drop_vars('lead_time')\n",
    "                           .isel(date=slice(0,10)))\n",
    "                fcst_p = fcst['hres_total_precipitation']\n",
    "                fcst_t = fcst['hres_temperature_2m']\n",
    "                fcst_s = fcst['hres_surface_net_solar_radiation']\n",
    "\n",
    "                precip = xr.concat([obs_weekly_p, obs_daily_p, fcst_p], dim='date')\n",
    "                temp   = xr.concat([obs_weekly_t, obs_daily_t, fcst_t], dim='date')\n",
    "                nsrad  = xr.concat([obs_weekly_s, obs_daily_s, fcst_s], dim='date')\n",
    "\n",
    "                if precip.shape[0] != EXPECTED_LEN:   # guard rail\n",
    "                    continue\n",
    "                if q_combined.shape[0] != EXPECTED_LEN:   # guard rail\n",
    "                    continue\n",
    "                    \n",
    "                # Flags\n",
    "                flags = np.concatenate([\n",
    "                    np.full(obs_weekly_p.date.size, 0),\n",
    "                    np.full(obs_daily_p.date.size, 1),\n",
    "                    np.full(fcst_p.date.size, 2)\n",
    "                ])\n",
    "                \n",
    "                sample = {\n",
    "                    'precip': precip.values.astype(np.float32),\n",
    "                    'temp':   temp.values.astype(np.float32),\n",
    "                    'net_solar': nsrad.values.astype(np.float32),\n",
    "                    'flag':   flags.astype(np.int8),\n",
    "                    'flow':   q_combined.values.astype(np.float32),\n",
    "                    'target': q_combined.values.astype(np.float32),\n",
    "                    'basin_id': gauge_id,\n",
    "                    'forecast_date': fcst_date.strftime('%Y-%m-%d')\n",
    "                }\n",
    "                samples.append(sample)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{gauge_id}] skip {fcst_date:%Y-%m-%d}: {e}\")\n",
    "\n",
    "        # write split‚Äëlevel NetCDF if any samples\n",
    "        if samples:\n",
    "            print(f\"[{gauge_id}] Writing to NetCDF...\")\n",
    "            ds = samples_to_xarray(samples)           # uses EXPECTED_LEN\n",
    "            outfile = SCRATCH / f'{split}_{gauge_id}.nc'\n",
    "            ds.to_netcdf(outfile)\n",
    "            out_files.append(str(outfile))\n",
    "\n",
    "    return out_files\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ‚ùπ  Small helper: samples ‚Üí xarray (single gauge, single split)\n",
    "# -----------------------------------------------------------\n",
    "def samples_to_xarray(samples):\n",
    "    n = len(samples)\n",
    "    dyn = np.zeros((n, EXPECTED_LEN, 4), np.float32)\n",
    "    tgt = np.zeros((n, EXPECTED_LEN, 1), np.float32)\n",
    "    bas = np.empty(n, 'U20')\n",
    "    fct = np.empty(n, 'U20')\n",
    "\n",
    "    for i, s in enumerate(samples):\n",
    "        dyn[i,:,0] = s['precip']\n",
    "        dyn[i,:,1] = s['temp']\n",
    "        dyn[i,:,2] = s['net_solar']\n",
    "        dyn_inputs[i, :, 3] = s['flag'].astype(np.float32)  # must be a float for LSTM\n",
    "        tgt[i,:,0] = s['target']\n",
    "        bas[i] = s['basin_id']\n",
    "        fct[i] = s['forecast_date']\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            \"dynamic_inputs\": (\n",
    "                [\"sample\", \"time\", \"feature\"],\n",
    "                dyn_inputs,\n",
    "                {\"feature\": [\"precip\", \"temp\", \"net_solar\", \"flag\"]}\n",
    "            ),\n",
    "            \"targets\": (\n",
    "                [\"sample\", \"time\", \"target\"],\n",
    "                targets,\n",
    "                {\"target\": [\"streamflow\"]}\n",
    "            ),\n",
    "            \"basin_id\": ([\"sample\"], basin_ids),\n",
    "            \"forecast_date\": ([\"sample\"], forecast_dates)\n",
    "        }\n",
    "    )\n",
    "    # Add metadata describing the meaning of flag values\n",
    "    ds.attrs[\"flag_description\"] = {\n",
    "        \"0\": \"weekly reanalysis (ERA5)\",\n",
    "        \"1\": \"daily reanalysis (ERA5)\",\n",
    "        \"2\": \"forecast (HRES)\"\n",
    "    }\n",
    "    return ds\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∫  Parent / driver\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    # ---------- (re-)create scratch folder ----------\n",
    "    if SCRATCH.exists():\n",
    "        shutil.rmtree(SCRATCH)\n",
    "    SCRATCH.mkdir(parents=True)\n",
    "\n",
    "    # ---------- build gauge list ----------\n",
    "    gauge_ids = df[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "    print('have gauge list')\n",
    "    # ---------- download streamflow (skip bad gauges) ----------\n",
    "    streamflows, skipped_gauges = get_or_download_streamflows(df)\n",
    "    import sys\n",
    "    print(str(sys.getsizeof(streamflows['01013500'])*1e-6)+' mb')\n",
    "    print('If this was tens or hundreds of MB, thats a lot to scatter all at once.')\n",
    "    print(f\"‚úÖ {len(streamflows)} gauges ready, ‚ùå {len(skipped_gauges)} skipped\")\n",
    "    print('Got the Gauges: starting parallelization')\n",
    "\n",
    "    # ---------- start Dask cluster ----------\n",
    "    cluster = LocalCluster(n_workers=64, threads_per_worker=2,\n",
    "                           processes=True, memory_limit=\"4GB\")\n",
    "    client  = Client(cluster)\n",
    "    # display(client)  # shows dashboard link in Jupyter\n",
    "\n",
    "    # ---------- dispatch only valid gauges ----------\n",
    "    # Scatter the streamflows dict ahead of time:\n",
    "    # Without scatter: every task pickles its df_streamflow argument ‚Üí the whole DataFrame travels across the network (or process boundary) every time.\n",
    "    # With scatter: you send each DataFrame once to each worker; tasks get a lightweight future (a pointer). Graphs shrink, network traffic drops.\n",
    "    # streamflows_future = client.scatter(streamflows, broadcast=True)\n",
    "    # streamflows_futures = client.scatter(streamflows)  # scatter returns dict of futures\n",
    "    # scatter the values individually (each streamflow gets its own future)\n",
    "\n",
    "    # Use Fewer Gauges First (Smoke Test)\n",
    "    some_gauges = list(streamflows.keys())[:3]\n",
    "    streamflows_futures = {\n",
    "        g: client.scatter(df, broadcast=True)\n",
    "        for g, df in {g: streamflows[g] for g in some_gauges}.items()\n",
    "    }\n",
    "\n",
    "    print(\"Submitting Dask tasks for\", len(streamflows_futures), \"gauges\")\n",
    "    futures = []\n",
    "    for g in some_gauges:\n",
    "        print(f\"[submit] gauge: {g}\")\n",
    "        fut = client.submit(process_gauge, g, streamflows_futures[g])\n",
    "        futures.append(fut)\n",
    "    print(\"All submits done\") # This will confirm the code has progressed beyond just building the task graph.\n",
    "\n",
    "\n",
    "    # futures = [client.submit(process_gauge, g, streamflows_future[g]) for g in streamflows.keys()] # only use the gauges that weren't skipped\n",
    "    # futures = [process_gauge(g, streamflows[g]) for g in streamflows.keys()]\n",
    "\n",
    "    # Below: This is telling Dask to record a performance profile during the execution of the tasks, and save it at dask_profile.html.\n",
    "    # with performance_report(filename=\"dask_profile.html\"):\n",
    "    #     results = compute(*futures)\n",
    "    # print('wrote report')\n",
    "    # if we dont have bokeh installed like we do, use without the profile.\n",
    "    # results = compute(*futures) # returns a tuple of results, not a list.\n",
    "    results = client.gather(futures)  # this gives a list of actual (out_files, stats) tuples\n",
    "\n",
    "    print(\"Finished Dask computation\")\n",
    "\n",
    "\n",
    "    # ---------- concatenate split‚Äëlevel files ----------\n",
    "    print('concatenating split-level files from parallelization')\n",
    "    for split in ['train','validation','test']:\n",
    "        files = sorted(SCRATCH.glob(f'{split}_*.nc'))\n",
    "        if not files: continue\n",
    "        ds = xr.open_mfdataset(files, combine='nested',\n",
    "                               concat_dim='sample',\n",
    "                               parallel=True)\n",
    "        ds.to_netcdf(\n",
    "            FINAL_OUT / f\"{split}_data_ERA5_HRES_CAMELS_unstandardized.nc\",\n",
    "            encoding={\n",
    "                \"dynamic_inputs\": {\"zlib\": True, \"complevel\": 4},\n",
    "                \"targets\":        {\"zlib\": True, \"complevel\": 4}\n",
    "            },\n",
    "        )\n",
    "        print(f\"[‚úì] wrote {split} set ({len(files)} gauges)\")\n",
    "\n",
    "    # ---------- at the very end ----------\n",
    "    if skipped_gauges:\n",
    "        print(\"Skipped gauges:\", \", \".join(skipped_gauges))\n",
    "        with open(FINAL_OUT / \"skipped_gauges.txt\", \"w\") as fp:\n",
    "            fp.write(\"\\n\".join(skipped_gauges))\n",
    "            \n",
    "    print(\"All done.  See NetCDFs in\", FINAL_OUT)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "#     # This is only needed if you're running the script directly from a terminal or !python ‚Äî not inside a notebook.\n",
    "#     # This design allows the file to serve two purposes:\n",
    "#     # Be run directly as a standalone program.\n",
    "#     # Be imported as a library/module into another script without executing the main logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d881fc6b-0fc8-4523-a048-48eeace7e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- build gauge list ----------\n",
    "# gauge_ids = df[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "# print('have gauge list')\n",
    "# # ---------- download streamflow (skip bad gauges) ----------\n",
    "# streamflows, skipped_gauges = get_or_download_streamflows(df)\n",
    "# streamflows\n",
    "# import sys\n",
    "# print(str(sys.getsizeof(streamflows['01013500'])*1e-6)+' mb')\n",
    "# print('If this was tens or hundreds of MB, thats a lot to scatter all at once.')\n",
    "# # print(f\"‚úÖ {len(streamflows)} gauges ready, ‚ùå {len(skipped_gauges)} skipped\")\n",
    "# # print('Got the Gauges: starting parallelization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc5ece7-cc88-4006-a4e1-33f8cb1ac68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamflows['01013500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc6b5f5-7d0f-41df-acfd-462bed7474c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- build gauge list ----------\n",
    "# gauge_ids = df[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "# print('have gauge list')\n",
    "# # ---------- download streamflow (skip bad gauges) ----------\n",
    "# streamflows, skipped_gauges = get_or_download_streamflows(df)\n",
    "# print(f\"‚úÖ {len(streamflows)} gauges ready, ‚ùå {len(skipped_gauges)} skipped\")\n",
    "# print('Got the Gauges: starting parallelization')\n",
    "# # ---------- start Dask cluster ----------\n",
    "# cluster = LocalCluster(n_workers=8, threads_per_worker=2,\n",
    "#                        processes=True, memory_limit=\"8GB\")\n",
    "# client  = Client(cluster)\n",
    "# # display(client)  # shows dashboard link in Jupyter\n",
    "\n",
    "# # ---------- dispatch only valid gauges ----------\n",
    "# # Scatter the streamflows dict ahead of time:\n",
    "# # Without scatter: every task pickles its df_streamflow argument ‚Üí the whole DataFrame travels across the network (or process boundary) every time.\n",
    "# # With scatter: you send each DataFrame once to each worker; tasks get a lightweight future (a pointer). Graphs shrink, network traffic drops.\n",
    "# # streamflows_future = client.scatter(streamflows, broadcast=True)\n",
    "# streamflows_futures = client.scatter(streamflows)  # scatter returns dict of futures\n",
    "\n",
    "# for g in streamflows.keys():\n",
    "#     print(g), print(streamflows_futures[g]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b47d76-5b62-4b28-83e7-3e0eaeda598c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have gauge list\n",
      "üîÅ Loading cached streamflows and skipped gauges...\n",
      "‚úÖ 587 gauges ready, ‚ùå 84 skipped\n",
      "Got the Gauges: starting parallelization\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c03b8-66f5-44bc-90d9-7ac54c1b73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# def to_xarray_dataset(samples, standardize=False, scaler=None):\n",
    "#     # Step 1: Count all time lengths (based on 'precip')\n",
    "#     lengths = [s['precip'].shape[0] for s in samples]\n",
    "#     length_counts = Counter(lengths)\n",
    "\n",
    "#     # Step 2: Infer most common sequence length\n",
    "#     EXPECTED_LEN, _ = length_counts.most_common(1)[0]\n",
    "\n",
    "#     REQUIRED_KEYS = ['precip', 'temp', 'net_solar', 'target', 'flag']\n",
    "\n",
    "#     # üîç Optional: Print samples with mismatched array lengths\n",
    "#     for s in samples:\n",
    "#         lengths = {k: s[k].shape[0] for k in REQUIRED_KEYS}\n",
    "#         if len(set(lengths.values())) > 1:\n",
    "#             print(f\"‚ö†Ô∏è Mismatched lengths for sample {s['forecast_date']} / {s['basin_id']}: {lengths}\")\n",
    "\n",
    "#     # Step 3: Keep only samples where ALL arrays match EXPECTED_LEN\n",
    "#     clean_samples = [\n",
    "#         s for s in samples\n",
    "#         if all(s[k].shape[0] == EXPECTED_LEN for k in REQUIRED_KEYS)\n",
    "#     ]\n",
    "\n",
    "#     dropped = len(samples) - len(clean_samples)\n",
    "#     if dropped > 0:\n",
    "#         print(f\"‚ö†Ô∏è Dropped {dropped} of {len(samples)} samples due to unexpected time length.\")\n",
    "\n",
    "#     if not clean_samples:\n",
    "#         raise ValueError(\"No valid samples with consistent length\")\n",
    "\n",
    "#     # ‚úÖ Missing before ‚Äî now added:\n",
    "#     n_samples = len(clean_samples)\n",
    "#     n_time = EXPECTED_LEN\n",
    "#     dyn_inputs = np.zeros((n_samples, n_time, 3), dtype=np.float32)\n",
    "#     targets = np.zeros((n_samples, n_time, 1), dtype=np.float32)\n",
    "#     basin_ids = np.empty(n_samples, dtype='U20')\n",
    "#     forecast_dates = np.empty(n_samples, dtype='U20')\n",
    "\n",
    "#     for i, s in enumerate(clean_samples):\n",
    "#         p = s['precip']\n",
    "#         t2 = s['temp']\n",
    "#         ns = s['net_solar']\n",
    "#         f = s['flag']\n",
    "#         t = s['target']\n",
    "\n",
    "#         if standardize:\n",
    "#             p  = standardize_tensor(p, *scaler['precip'])\n",
    "#             t2 = standardize_tensor(t2, *scaler['temp'])\n",
    "#             ns = standardize_tensor(ns, *scaler['net_solar'])\n",
    "#             f = standardize_tensor(ns, *scaler['flag'])\n",
    "#             t  = standardize_tensor(t, *scaler['target'])\n",
    "\n",
    "#         dyn_inputs[i, :, 0] = p\n",
    "#         dyn_inputs[i, :, 1] = t2\n",
    "#         dyn_inputs[i, :, 2] = ns\n",
    "#         dyn_inputs[i, :, 3] = flag\n",
    "#         targets[i, :, 0] = t\n",
    "#         basin_ids[i] = s['basin_id']\n",
    "#         forecast_dates[i] = s['forecast_date']\n",
    "\n",
    "#     return xr.Dataset(\n",
    "#         {\n",
    "#             \"dynamic_inputs\": (\n",
    "#                 [\"sample\", \"time\", \"feature\"],\n",
    "#                 dyn_inputs,\n",
    "#                 {\"feature\": [\"precip\", \"temp\", \"net_solar\",\"flag\"]}\n",
    "#             ),\n",
    "#             \"targets\": (\n",
    "#                 [\"sample\", \"time\", \"target\"],\n",
    "#                 targets,\n",
    "#                 {\"target\": [\"streamflow\"]}\n",
    "#             ),\n",
    "#             \"basin_id\": ([\"sample\"], basin_ids),\n",
    "#             \"forecast_date\": ([\"sample\"], forecast_dates)\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f859a19-33a4-46cd-9a18-40474bf194a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write 2 NetCDF files (train + validation) unstandardized\n",
    "# for split in [\"train\", \"validation\", \"test\"]:\n",
    "# # split=\"test\"\n",
    "#     std = False\n",
    "#     ds = to_xarray_dataset(\n",
    "#         dataset_store[split],\n",
    "#         standardize=False,\n",
    "#         scaler=scaler\n",
    "#     )\n",
    "#     suffix = \"standardized\" if std else \"unstandardized\"\n",
    "#     fname = f\"{split}_data_ERA5_HRES_CAMELS_{suffix}.nc\"\n",
    "#     ds.to_netcdf('/Projects/HydroMet/currierw/HRES_processed/'+fname)\n",
    "#     print(f\"Saved: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87424320-f862-4cf1-ac8a-f1120dcbb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset_store['train'][0]['precip'].shape[0])\n",
    "# print(dataset_store['train'][0]['temp'].shape[0])\n",
    "# print(dataset_store['train'][0]['net_solar'].shape[0])\n",
    "# print(dataset_store['train'][0]['flow'].shape[0])\n",
    "# print(dataset_store['train'][0]['target'].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2dbcff-3b20-41c3-bfa0-bfb638d5ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds=xr.open_dataset('/Projects/HydroMet/currierw/HRES_processed/train_data_ERA5_HRES_CAMELS_unstandardized.nc')\n",
    "# ds['dynamic_inputs']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology editable",
   "language": "Python",
   "name": "neuralhydrology_edit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
