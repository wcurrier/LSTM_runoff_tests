{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d137b1ee-57f7-4a08-b0ea-971a80231d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Logical CPU cores :\", multiprocessing.cpu_count())\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"CUDA device      :\", torch.cuda.get_device_name(0))\n",
    "#     print(\"GPU capability   :\", torch.cuda.get_device_capability(0))\n",
    "# else:\n",
    "#     print(\"No CUDA‑capable GPU detected\")\n",
    "\n",
    "# print(\"RAM (GB total)   :\", round(psutil.virtual_memory().total / 1e9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c45753f-0444-450a-91fe-8ae19f42b843",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Loading cached streamflows and skipped gauges...\n",
      "Loaded 587 gauges. Skipped 84.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41303 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting final batch of 150 tasks...\n",
      "Waiting for results and saving intermediate files...\n",
      "Final save of remaining results...\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01013500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01013500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01013500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01022500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01022500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01022500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01030500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01030500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01030500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01031500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01031500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01031500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01047000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01047000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01047000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01052500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01052500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01052500.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01054200.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01054200.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01054200.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01055000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01055000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01055000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01057000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01057000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01057000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/train_01073000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/validation_01073000.nc\n",
      "Wrote /Projects/HydroMet/currierw/HRES_processed_tmp/test_01073000.nc\n",
      "Done. Total time: 0.1 minutes\n",
      "Results saved to /Projects/HydroMet/currierw/HRES_processed_tmp\n"
     ]
    }
   ],
   "source": [
    "# preprocess_LSTM_dask.py\n",
    "# -----------------------------------------------------------\n",
    "# ❶  Imports & Dask cluster\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd, numpy as np, xarray as xr, geopandas as gpd\n",
    "import traceback, sys, dask, pickle, torch, zarr\n",
    "import multiprocessing, psutil, os, tempfile, json, shutil, time, urllib.error\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import LocalCluster, Client, performance_report, wait, as_completed\n",
    "from IPython.display import display # Dask dashboard inside the notebook itself:\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from data_access import era5, hres\n",
    "from utils import build_sample, samples_to_xarray, build_sample_wrapped, get_usgs_streamflow, get_or_download_streamflows\n",
    "# from config import FORECAST_BLOCKS, SCRATCH\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# paths & constants -------------------------------------------------\n",
    "BASE_OBS  = Path('/Projects/HydroMet/currierw/ERA5_LAND')\n",
    "BASE_FCST = Path('/Projects/HydroMet/currierw/HRES')\n",
    "SCRATCH   = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')\n",
    "FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "STREAMFLOW_PATH = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "\n",
    "FORECAST_BLOCKS = {\n",
    "    \"train\":      pd.date_range('2016-01-01', '2020-09-30', freq='5D'),\n",
    "    \"validation\": pd.date_range('2020-10-01', '2022-09-30', freq='5D'),\n",
    "    \"test\":       pd.date_range('2022-10-01', '2024-09-30', freq='5D'),\n",
    "}\n",
    "EXPECTED_LEN = 106\n",
    "\n",
    "\n",
    "# ---------------------------- Main driver -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 2_000\n",
    "    SAVE_INTERVAL = 2_000\n",
    "\n",
    "    # ---- Clear scratch output folder ----\n",
    "    if SCRATCH.exists():\n",
    "        shutil.rmtree(SCRATCH)\n",
    "    SCRATCH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Load gauges ----\n",
    "    gdf = gpd.read_file('/Projects/HydroMet/currierw/Caravan-Jan25-csv/shapefiles/camels/camels_basin_shapes.shp')\n",
    "    gauge_ids = gdf[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "    streamflows, skipped = get_or_download_streamflows(gdf, STREAMFLOW_PATH)\n",
    "\n",
    "    print(f\"Loaded {len(streamflows)} gauges. Skipped {len(skipped)}.\")\n",
    "\n",
    "    # ---- Dask cluster setup ----\n",
    "    module_path = '/home/wcurrier/LSTM_runoff_tests'\n",
    "\n",
    "    def add_module_path(dask_worker):\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, module_path)\n",
    "\n",
    "    cluster = LocalCluster(n_workers=16, threads_per_worker=2, memory_limit=\"8GB\")\n",
    "    # cluster.adapt(minimum=8, maximum=32)\n",
    "    client = Client(cluster)\n",
    "    client.run(add_module_path)\n",
    "\n",
    "    # ---- Task submission in batches ----\n",
    "\n",
    "    ds_era5 = era5()\n",
    "    ds_hres = hres()\n",
    "\n",
    "    all_futures = []\n",
    "    pending = []\n",
    "\n",
    "    for gauge_id in list(streamflows.keys())[:10]:\n",
    "        dfQ = streamflows[gauge_id]\n",
    "        for split, dates in FORECAST_BLOCKS.items():\n",
    "            for d in dates[:5]:\n",
    "                future = client.submit(\n",
    "                    build_sample_wrapped, gauge_id, split, d, dfQ, ds_era5, ds_hres, pure=False\n",
    "                )\n",
    "                pending.append(future)\n",
    "\n",
    "                if len(pending) >= BATCH_SIZE:\n",
    "                    print(f\"Submitting batch of {len(pending)} tasks...\")\n",
    "                    wait(pending, timeout=\"10m\")\n",
    "                    all_futures.extend(pending)\n",
    "                    pending = []\n",
    "\n",
    "    # Final pending batch\n",
    "    if pending:\n",
    "        print(f\"Submitting final batch of {len(pending)} tasks...\")\n",
    "        wait(pending, timeout=\"10m\")\n",
    "        all_futures.extend(pending)\n",
    "\n",
    "    # ---- Collect results as they complete ----\n",
    "    print(\"Waiting for results and saving intermediate files...\")\n",
    "    start_time = time.time()\n",
    "    grouped = defaultdict(list)\n",
    "    counter = 0\n",
    "\n",
    "    for future, result in as_completed(all_futures, with_results=True):\n",
    "        if result is not None:\n",
    "            grouped[(result[\"basin_id\"], result[\"split\"])].append(result)\n",
    "            counter += 1\n",
    "\n",
    "        if counter % SAVE_INTERVAL == 0 and counter > 0:\n",
    "            print(f\"[{counter}] Saving intermediate results at {time.time() - start_time:.1f}s\")\n",
    "            for (gauge, split), samples in grouped.items():\n",
    "                ds = samples_to_xarray(samples)\n",
    "                out = SCRATCH / f\"{split}_{gauge}_{counter}.nc\"\n",
    "\n",
    "                if ds.dynamic_inputs.isnull().any() or ds.targets.isnull().any():\n",
    "                    print(f\"[{split}][{gauge}] contains NaNs — skipping {out.name}\")\n",
    "                    continue\n",
    "\n",
    "                ds.to_netcdf(out)\n",
    "                # print(\"Wrote\", out)\n",
    "\n",
    "            grouped.clear()\n",
    "\n",
    "    # ---- Final save ----\n",
    "    print(\"Final save of remaining results...\")\n",
    "    summary = []\n",
    "\n",
    "    for (gauge, split), samples in grouped.items():\n",
    "        ds = samples_to_xarray(samples)\n",
    "        out = SCRATCH / f\"{split}_{gauge}.nc\"\n",
    "\n",
    "        if ds.dynamic_inputs.isnull().any() or ds.targets.isnull().any():\n",
    "            print(f\"[{split}][{gauge}] contains NaNs — skipping final save\")\n",
    "            continue\n",
    "\n",
    "        ds.to_netcdf(out)\n",
    "        print(\"Wrote\", out)\n",
    "\n",
    "        summary.append({\n",
    "            \"gauge_id\": gauge,\n",
    "            \"split\": split,\n",
    "            \"n_samples\": len(samples),\n",
    "            \"filename\": out.name\n",
    "        })\n",
    "\n",
    "    # ---- Summary output ----\n",
    "    pd.DataFrame(summary).to_csv(\"processed_summary.csv\", index=False)\n",
    "    print(f\"Done. Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"Results saved to {SCRATCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafb288-9c8c-4db9-a59e-0ecfcb2cc319",
   "metadata": {},
   "source": [
    "# Concatenate indivudal files from parallel output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac797ee9-6771-4a73-8daf-eb5462ace704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# SCRATCH = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')\n",
    "# FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "# # Step 1: List of splits\n",
    "# splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# for split in splits:\n",
    "#     files = sorted(SCRATCH.glob(f\"{split}_*.nc\"))\n",
    "#     valid_datasets = []\n",
    "\n",
    "#     print(f\"\\n🧪 Checking {len(files)} files for '{split}' split...\")\n",
    "\n",
    "#     for f in files:\n",
    "#         ds = xr.open_dataset(f)\n",
    "\n",
    "#         # Check for NaNs\n",
    "#         has_nan = (\n",
    "#             np.isnan(ds[\"dynamic_inputs\"]).any() or\n",
    "#             np.isnan(ds[\"targets\"]).any()\n",
    "#         )\n",
    "\n",
    "#         if has_nan:\n",
    "#             print(f\"⚠️ Skipping file with NaNs: {f.name}\")\n",
    "#             continue\n",
    "\n",
    "#         valid_datasets.append(ds)\n",
    "\n",
    "#     if not valid_datasets:\n",
    "#         print(f\"❌ No valid files found for split: {split}\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"✅ {len(valid_datasets)} valid files found for '{split}'\")\n",
    "\n",
    "#     # Step 2: Concatenate along the sample dimension\n",
    "#     combined = xr.concat(valid_datasets, dim=\"sample\")\n",
    "\n",
    "#     # Step 3: Write out combined file\n",
    "#     output_path = FINAL_OUT / f\"{split}.nc\"\n",
    "#     combined.to_netcdf(output_path)\n",
    "#     print(f\"📦 Wrote combined dataset for '{split}' → {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff4f7c-5689-4e53-9fb1-f9d40956f367",
   "metadata": {},
   "source": [
    "# Cleanup scratch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b47d76-5b62-4b28-83e7-3e0eaeda598c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Optional cleanup\n",
    "# for f in SCRATCH.glob(\"*.nc\"):\n",
    "#     f.unlink()  # or f.rename(SCRATCH / \"archive\" / f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology editable new",
   "language": "Python",
   "name": "neuralhydrology_edit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
