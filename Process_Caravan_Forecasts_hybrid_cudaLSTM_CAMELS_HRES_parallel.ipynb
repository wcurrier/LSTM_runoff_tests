{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fa8e56-6854-4a93-928f-61fdae5442b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, LocalCluster, wait, as_completed\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_access\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m era5, hres\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_sample, samples_to_xarray, load_streamflow_for_gauge,  build_sample_from_disk, get_usgs_streamflow, get_or_download_streamflows\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# from config import FORECAST_BLOCKS, SCRATCH\u001b[39;00m\n",
      "File \u001b[0;32m~/LSTM_runoff_tests/utils.py:38\u001b[0m\n\u001b[1;32m     21\u001b[0m     ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m     22\u001b[0m         data_vars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     23\u001b[0m             dynamic_inputs\u001b[38;5;241m=\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m], dyn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m         )\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sample\u001b[39m(gauge_id: \u001b[38;5;28mstr\u001b[39m, split: \u001b[38;5;28mstr\u001b[39m, fcst_date: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mTimestamp, df_streamflow):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    Returns dict | None.  No huge objects passed around.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return one sample dict or None.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# preprocess_LSTM_dask.py\n",
    "# -----------------------------------------------------------\n",
    "# ‚ù∂  Imports & Dask cluster\n",
    "# -----------------------------------------------------------\n",
    "import torch, zarr\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import LocalCluster, Client, performance_report\n",
    "from IPython.display import display # Dask dashboard inside the notebook itself:\n",
    "import traceback, sys, dask, pickle\n",
    "import pandas as pd, numpy as np, xarray as xr\n",
    "import os, multiprocessing, torch, psutil, os, tempfile, json, shutil, time\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from functools import lru_cache\n",
    "from collections import defaultdict\n",
    "from dask.distributed import Client, LocalCluster, wait, as_completed\n",
    "\n",
    "from data_access import era5, hres\n",
    "from utils import build_sample, samples_to_xarray, load_streamflow_for_gauge,  build_sample_from_disk, get_usgs_streamflow, get_or_download_streamflows\n",
    "# from config import FORECAST_BLOCKS, SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137b1ee-57f7-4a08-b0ea-971a80231d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Logical CPU cores :\", multiprocessing.cpu_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device      :\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU capability   :\", torch.cuda.get_device_capability(0))\n",
    "else:\n",
    "    print(\"No CUDA‚Äëcapable GPU detected\")\n",
    "\n",
    "print(\"RAM (GB total)   :\", round(psutil.virtual_memory().total / 1e9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45753f-0444-450a-91fe-8ae19f42b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# paths & constants -------------------------------------------------\n",
    "BASE_OBS  = Path('/Projects/HydroMet/currierw/ERA5_LAND')\n",
    "BASE_FCST = Path('/Projects/HydroMet/currierw/HRES')\n",
    "SCRATCH   = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')\n",
    "FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "\n",
    "FORECAST_BLOCKS = {\n",
    "    \"train\":      pd.date_range('2016-01-01', '2020-09-30', freq='5D'),\n",
    "    \"validation\": pd.date_range('2020-10-01', '2022-09-30', freq='5D'),\n",
    "    \"test\":       pd.date_range('2022-10-01', '2024-09-30', freq='5D'),\n",
    "}\n",
    "EXPECTED_LEN = 106\n",
    "\n",
    "\n",
    "# ---------------------------- Main driver -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    BATCH_SIZE = 2_000\n",
    "    SAVE_INTERVAL = 2_000\n",
    "\n",
    "    # ---- Clear scratch output folder ----\n",
    "    if SCRATCH.exists():\n",
    "        shutil.rmtree(SCRATCH)\n",
    "    SCRATCH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Load gauges ----\n",
    "    gdf = gpd.read_file('/Projects/HydroMet/currierw/Caravan-Jan25-csv/shapefiles/camels/camels_basin_shapes.shp')\n",
    "    gauge_ids = gdf[\"gauge_id\"].str.split(\"_\").str[-1].tolist()\n",
    "    streamflows, skipped = get_or_download_streamflows(gdf)\n",
    "\n",
    "    print(f\"Loaded {len(streamflows)} gauges. Skipped {len(skipped)}.\")\n",
    "\n",
    "    # ---- Dask cluster setup ----\n",
    "    module_path = '/home/wcurrier/LSTM_runoff_tests'\n",
    "\n",
    "    def add_module_path(dask_worker):\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, module_path)\n",
    "\n",
    "    cluster = LocalCluster(n_workers=16, threads_per_worker=2, memory_limit=\"8GB\")\n",
    "    cluster.adapt(minimum=8, maximum=32)\n",
    "    client = Client(cluster)\n",
    "    client.run(add_module_path)\n",
    "\n",
    "    # ---- Task submission in batches ----\n",
    "    all_futures = []\n",
    "    pending = []\n",
    "\n",
    "    for gauge_id in streamflow[:10]s:\n",
    "        for split, dates in FORECAST_BLOCKS.items():\n",
    "            for d in dates[:5]:\n",
    "                future = client.submit(build_sample_from_disk, gauge_id, split, d, pure=False)\n",
    "                pending.append(future)\n",
    "\n",
    "                if len(pending) >= BATCH_SIZE:\n",
    "                    print(f\"Submitting batch of {len(pending)} tasks...\")\n",
    "                    wait(pending, timeout=\"10m\")\n",
    "                    all_futures.extend(pending)\n",
    "                    pending = []\n",
    "\n",
    "    # Final pending batch\n",
    "    if pending:\n",
    "        print(f\"Submitting final batch of {len(pending)} tasks...\")\n",
    "        wait(pending, timeout=\"10m\")\n",
    "        all_futures.extend(pending)\n",
    "\n",
    "    # ---- Collect results as they complete ----\n",
    "    print(\"Waiting for results and saving intermediate files...\")\n",
    "    start_time = time.time()\n",
    "    grouped = defaultdict(list)\n",
    "    counter = 0\n",
    "\n",
    "    for future, result in as_completed(all_futures, with_results=True):\n",
    "        if result is not None:\n",
    "            grouped[(result[\"basin_id\"], result[\"split\"])].append(result)\n",
    "            counter += 1\n",
    "\n",
    "        if counter % SAVE_INTERVAL == 0 and counter > 0:\n",
    "            print(f\"[{counter}] Saving intermediate results at {time.time() - start_time:.1f}s\")\n",
    "            for (gauge, split), samples in grouped.items():\n",
    "                ds = samples_to_xarray(samples)\n",
    "                out = SCRATCH / f\"{split}_{gauge}_{counter}.nc\"\n",
    "\n",
    "                if ds.dynamic_inputs.isnull().any() or ds.targets.isnull().any():\n",
    "                    print(f\"[{split}][{gauge}] contains NaNs ‚Äî skipping {out.name}\")\n",
    "                    continue\n",
    "\n",
    "                ds.to_netcdf(out)\n",
    "                print(\"Wrote\", out)\n",
    "\n",
    "            grouped.clear()\n",
    "\n",
    "    # ---- Final save ----\n",
    "    print(\"Final save of remaining results...\")\n",
    "    summary = []\n",
    "\n",
    "    for (gauge, split), samples in grouped.items():\n",
    "        ds = samples_to_xarray(samples)\n",
    "        out = SCRATCH / f\"{split}_{gauge}.nc\"\n",
    "\n",
    "        if ds.dynamic_inputs.isnull().any() or ds.targets.isnull().any():\n",
    "            print(f\"[{split}][{gauge}] contains NaNs ‚Äî skipping final save\")\n",
    "            continue\n",
    "\n",
    "        ds.to_netcdf(out)\n",
    "        print(\"Wrote\", out)\n",
    "\n",
    "        summary.append({\n",
    "            \"gauge_id\": gauge,\n",
    "            \"split\": split,\n",
    "            \"n_samples\": len(samples),\n",
    "            \"filename\": out.name\n",
    "        })\n",
    "\n",
    "    # ---- Summary output ----\n",
    "    pd.DataFrame(summary).to_csv(SCRATCH / \"processed_summary.csv\", index=False)\n",
    "    print(f\"Done. Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"Results saved to {SCRATCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafb288-9c8c-4db9-a59e-0ecfcb2cc319",
   "metadata": {},
   "source": [
    "# Code to concatenate indivudal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac797ee9-6771-4a73-8daf-eb5462ace704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xarray as xr\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# SCRATCH = Path('/Projects/HydroMet/currierw/HRES_processed_tmp')\n",
    "# FINAL_OUT = Path('/Projects/HydroMet/currierw/HRES_processed')\n",
    "# # Step 1: List of splits\n",
    "# splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# for split in splits:\n",
    "#     files = sorted(SCRATCH.glob(f\"{split}_*.nc\"))\n",
    "#     valid_datasets = []\n",
    "\n",
    "#     print(f\"\\nüß™ Checking {len(files)} files for '{split}' split...\")\n",
    "\n",
    "#     for f in files:\n",
    "#         ds = xr.open_dataset(f)\n",
    "\n",
    "#         # Check for NaNs\n",
    "#         has_nan = (\n",
    "#             np.isnan(ds[\"dynamic_inputs\"]).any() or\n",
    "#             np.isnan(ds[\"targets\"]).any()\n",
    "#         )\n",
    "\n",
    "#         if has_nan:\n",
    "#             print(f\"‚ö†Ô∏è Skipping file with NaNs: {f.name}\")\n",
    "#             continue\n",
    "\n",
    "#         valid_datasets.append(ds)\n",
    "\n",
    "#     if not valid_datasets:\n",
    "#         print(f\"‚ùå No valid files found for split: {split}\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"‚úÖ {len(valid_datasets)} valid files found for '{split}'\")\n",
    "\n",
    "#     # Step 2: Concatenate along the sample dimension\n",
    "#     combined = xr.concat(valid_datasets, dim=\"sample\")\n",
    "\n",
    "#     # Step 3: Write out combined file\n",
    "#     output_path = FINAL_OUT / f\"{split}.nc\"\n",
    "#     combined.to_netcdf(output_path)\n",
    "#     print(f\"üì¶ Wrote combined dataset for '{split}' ‚Üí {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b47d76-5b62-4b28-83e7-3e0eaeda598c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Optional cleanup\n",
    "# for f in SCRATCH.glob(\"*.nc\"):\n",
    "#     f.unlink()  # or f.rename(SCRATCH / \"archive\" / f.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralhydrology editable new",
   "language": "Python",
   "name": "neuralhydrology_edit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
